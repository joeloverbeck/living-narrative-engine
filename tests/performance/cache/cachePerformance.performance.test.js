/**
 * @file Performance tests for unified cache infrastructure
 * Tests cache performance under various load conditions
 */

import { describe, it, expect, beforeEach, afterEach } from '@jest/globals';
import { createTestBed } from '../../common/testBed.js';
import { UnifiedCache } from '../../../src/cache/UnifiedCache.js';
import { CacheInvalidationManager } from '../../../src/cache/CacheInvalidationManager.js';
import { CacheMetrics } from '../../../src/cache/CacheMetrics.js';

describe('Cache Performance Tests', () => {
  let testBed;
  let mockLogger;
  let mockValidatedEventDispatcher;

  beforeEach(() => {
    testBed = createTestBed();
    mockLogger = testBed.createMockLogger();
    mockValidatedEventDispatcher = testBed.createMock('validatedEventDispatcher', [
      'on', 'off', 'dispatch'
    ]);
  });

  afterEach(() => {
    testBed.cleanup();
  });

  describe('UnifiedCache Performance', () => {
    it('should handle high-volume set operations efficiently', () => {
      const cache = new UnifiedCache({ logger: mockLogger }, {
        maxSize: 10000,
        evictionPolicy: 'lru'
      });

      const startTime = process.hrtime.bigint();
      const itemCount = 5000;

      for (let i = 0; i < itemCount; i++) {
        cache.set(`key${i}`, { id: i, value: `value${i}`, timestamp: Date.now() });
      }

      const endTime = process.hrtime.bigint();
      const duration = Number(endTime - startTime) / 1000000; // Convert to milliseconds

      // Should complete within 500ms (target: ~10,000 ops/sec)
      expect(duration).toBeLessThan(500);
      
      const metrics = cache.getMetrics();
      expect(metrics.stats.sets).toBe(itemCount);
      expect(metrics.size).toBeLessThanOrEqual(10000);

      console.log(`Set Performance: ${itemCount} operations in ${duration.toFixed(2)}ms (${(itemCount / duration * 1000).toFixed(0)} ops/sec)`);
    });

    it('should handle high-volume get operations efficiently', () => {
      const cache = new UnifiedCache({ logger: mockLogger }, {
        maxSize: 10000,
        evictionPolicy: 'lru'
      });

      // Pre-populate cache
      const itemCount = 2000;
      for (let i = 0; i < itemCount; i++) {
        cache.set(`key${i}`, { id: i, value: `value${i}` });
      }

      const startTime = process.hrtime.bigint();
      
      // Perform random gets
      for (let i = 0; i < itemCount * 2; i++) {
        const keyIndex = Math.floor(Math.random() * itemCount);
        cache.get(`key${keyIndex}`);
      }

      const endTime = process.hrtime.bigint();
      const duration = Number(endTime - startTime) / 1000000;

      // Should complete within 200ms
      expect(duration).toBeLessThan(200);

      const metrics = cache.getMetrics();
      expect(metrics.stats.hits).toBeGreaterThan(0);

      console.log(`Get Performance: ${itemCount * 2} operations in ${duration.toFixed(2)}ms (${(itemCount * 2 / duration * 1000).toFixed(0)} ops/sec)`);
    });

    it('should handle mixed read/write workload efficiently', () => {
      const cache = new UnifiedCache({ logger: mockLogger }, {
        maxSize: 5000,
        evictionPolicy: 'lru'
      });

      const operations = 10000;
      const startTime = process.hrtime.bigint();

      for (let i = 0; i < operations; i++) {
        if (i % 3 === 0) {
          // Write operation
          cache.set(`key${i % 1000}`, { id: i, value: `value${i}` });
        } else {
          // Read operation
          cache.get(`key${Math.floor(Math.random() * 1000)}`);
        }
      }

      const endTime = process.hrtime.bigint();
      const duration = Number(endTime - startTime) / 1000000;

      // Should complete within 800ms
      expect(duration).toBeLessThan(800);

      const metrics = cache.getMetrics();
      expect(metrics.stats.sets + metrics.stats.hits + metrics.stats.misses).toBe(operations);

      console.log(`Mixed Workload: ${operations} operations in ${duration.toFixed(2)}ms (${(operations / duration * 1000).toFixed(0)} ops/sec)`);
    });

    it('should compare performance across eviction strategies', () => {
      const strategies = ['lru', 'lfu', 'fifo'];
      const results = {};

      strategies.forEach(strategy => {
        const cache = new UnifiedCache({ logger: mockLogger }, {
          maxSize: 1000,
          evictionPolicy: strategy
        });

        const operations = 2000;
        const startTime = process.hrtime.bigint();

        // Mix of operations that will trigger evictions
        for (let i = 0; i < operations; i++) {
          if (i < 1200) {
            cache.set(`key${i}`, { value: i });
          } else {
            // Access some existing keys (affects LRU/LFU)
            cache.get(`key${i % 600}`);
            // Add new keys (triggers eviction)
            cache.set(`new${i}`, { value: i });
          }
        }

        const endTime = process.hrtime.bigint();
        const duration = Number(endTime - startTime) / 1000000;

        results[strategy] = {
          duration,
          throughput: operations / duration * 1000,
          metrics: cache.getMetrics()
        };

        expect(duration).toBeLessThan(1000); // All strategies should be under 1 second
      });

      // Log performance comparison
      console.log('\nEviction Strategy Performance Comparison:');
      strategies.forEach(strategy => {
        const result = results[strategy];
        console.log(`${strategy.toUpperCase()}: ${result.duration.toFixed(2)}ms, ${result.throughput.toFixed(0)} ops/sec`);
      });

      // Verify all strategies maintain cache size limit
      strategies.forEach(strategy => {
        expect(results[strategy].metrics.size).toBeLessThanOrEqual(1000);
      });
    });
  });

  describe('Cache Invalidation Performance', () => {
    it('should handle large-scale pattern invalidation efficiently', () => {
      const invalidationManager = new CacheInvalidationManager({
        logger: mockLogger,
        validatedEventDispatcher: mockValidatedEventDispatcher,
      });

      // Create multiple caches
      const caches = [];
      for (let i = 0; i < 10; i++) {
        const cache = new UnifiedCache({ logger: mockLogger }, {
          maxSize: 1000,
          evictionPolicy: 'lru'
        });

        // Populate each cache
        for (let j = 0; j < 500; j++) {
          cache.set(`entity:type${i}:item${j}`, { type: i, id: j });
        }

        invalidationManager.registerCache(`cache${i}`, cache, {
          entityTypes: [`type${i}`]
        });
        caches.push(cache);
      }

      const startTime = process.hrtime.bigint();
      
      // Invalidate pattern across all caches
      const result = invalidationManager.invalidatePattern('entity:type');

      const endTime = process.hrtime.bigint();
      const duration = Number(endTime - startTime) / 1000000;

      // Should complete within 100ms
      expect(duration).toBeLessThan(100);

      // Calculate total invalidated entries
      const totalInvalidated = Object.values(result).reduce((sum, r) =>
        sum + (r.success ? r.invalidated : 0), 0
      );
      expect(totalInvalidated).toBeGreaterThan(0);

      console.log(`Pattern Invalidation: ${totalInvalidated} entries across ${caches.length} caches in ${duration.toFixed(2)}ms`);

      // Cleanup
      invalidationManager.destroy();
    });

    it('should handle concurrent invalidation requests efficiently', async () => {
      const invalidationManager = new CacheInvalidationManager({
        logger: mockLogger,
        validatedEventDispatcher: mockValidatedEventDispatcher,
      });

      const cache = new UnifiedCache({ logger: mockLogger }, {
        maxSize: 5000,
        evictionPolicy: 'lru'
      });

      // Populate cache with different patterns
      const patterns = ['user:', 'item:', 'game:', 'system:', 'temp:'];
      patterns.forEach((pattern, i) => {
        for (let j = 0; j < 200; j++) {
          cache.set(`${pattern}${i}-${j}`, { pattern, index: j });
        }
      });

      invalidationManager.registerCache('concurrent-cache', cache);

      const startTime = process.hrtime.bigint();

      // Create concurrent invalidation requests
      const promises = patterns.map(pattern =>
        Promise.resolve().then(() =>
          invalidationManager.invalidatePattern(pattern)
        )
      );

      const results = await Promise.all(promises);

      const endTime = process.hrtime.bigint();
      const duration = Number(endTime - startTime) / 1000000;

      // Should complete within 50ms
      expect(duration).toBeLessThan(50);

      // Calculate total invalidated across all results
      const totalInvalidated = results.reduce((sum, result) => {
        return sum + Object.values(result).reduce((innerSum, r) =>
          innerSum + (r.success ? r.invalidated : 0), 0
        );
      }, 0);
      expect(totalInvalidated).toBeGreaterThan(0);

      console.log(`Concurrent Invalidation: ${patterns.length} patterns, ${totalInvalidated} total entries in ${duration.toFixed(2)}ms`);

      invalidationManager.destroy();
    });
  });

  describe('Cache Metrics Performance', () => {
    it('should collect metrics efficiently across many caches', () => {
      const metricsService = new CacheMetrics({ logger: mockLogger });
      
      // Create and register many caches
      const cacheCount = 50;
      const caches = [];
      
      for (let i = 0; i < cacheCount; i++) {
        const cache = new UnifiedCache({ logger: mockLogger }, {
          maxSize: 100,
          evictionPolicy: ['lru', 'lfu', 'fifo'][i % 3]
        });

        // Add some data to each cache
        for (let j = 0; j < 20; j++) {
          cache.set(`cache${i}:key${j}`, { cache: i, key: j });
        }

        metricsService.registerCache(`cache-${i}`, cache, {
          category: `category-${i % 5}`,
          description: `Cache ${i}`
        });

        caches.push(cache);
      }

      const startTime = process.hrtime.bigint();
      
      // Collect aggregated metrics
      const aggregated = metricsService.getAggregatedMetrics();

      const endTime = process.hrtime.bigint();
      const duration = Number(endTime - startTime) / 1000000;

      // Should complete within 100ms
      expect(duration).toBeLessThan(100);
      expect(aggregated.cacheCount).toBe(cacheCount);
      expect(aggregated.totalSize).toBe(cacheCount * 20);

      console.log(`Metrics Collection: ${cacheCount} caches in ${duration.toFixed(2)}ms`);

      metricsService.destroy();
    });

    it('should handle high-frequency metrics collection efficiently', () => {
      const metricsService = new CacheMetrics({ logger: mockLogger }, {
        collectInterval: 10, // Very frequent collection
        retentionHours: 1
      });

      const cache = new UnifiedCache({ logger: mockLogger }, {
        maxSize: 1000,
        evictionPolicy: 'lru'
      });

      metricsService.registerCache('high-frequency-cache', cache);

      // Start automatic collection
      metricsService.startCollection(10);

      const startTime = process.hrtime.bigint();
      
      // Simulate workload while metrics are being collected
      return new Promise((resolve) => {
        setTimeout(() => {
          // Perform cache operations
          for (let i = 0; i < 1000; i++) {
            cache.set(`key${i}`, { value: i });
            if (i % 10 === 0) {
              cache.get(`key${Math.floor(Math.random() * i + 1)}`);
            }
          }

          const endTime = process.hrtime.bigint();
          const duration = Number(endTime - startTime) / 1000000;

          // Stop collection
          metricsService.stopCollection();

          // Check that metrics were collected
          const history = metricsService.getHistoricalData();
          expect(history.length).toBeGreaterThan(0);

          // Operations should complete despite frequent metrics collection
          expect(duration).toBeLessThan(200);

          console.log(`High-Frequency Metrics: 1000 operations with frequent collection in ${duration.toFixed(2)}ms`);
          console.log(`Collected ${history.length} metric snapshots`);

          metricsService.destroy();
          resolve();
        }, 100); // Let collection run for 100ms
      });
    });

    it('should analyze performance efficiently for large datasets', () => {
      const metricsService = new CacheMetrics({ logger: mockLogger });
      
      // Create caches with varying performance characteristics
      const scenarios = [
        { hitRate: 0.9, evictions: 5, strategy: 'lru', size: 100 },
        { hitRate: 0.7, evictions: 20, strategy: 'lfu', size: 80 },
        { hitRate: 0.5, evictions: 50, strategy: 'fifo', size: 60 },
        { hitRate: 0.3, evictions: 100, strategy: 'lru', size: 40 },
        { hitRate: 0.1, evictions: 200, strategy: 'lfu', size: 20 }
      ];

      scenarios.forEach((scenario, i) => {
        const mockCache = {
          getMetrics: jest.fn().mockReturnValue({
            size: scenario.size,
            maxSize: 100,
            hitRate: scenario.hitRate,
            stats: {
              hits: Math.floor(1000 * scenario.hitRate),
              misses: Math.floor(1000 * (1 - scenario.hitRate)),
              evictions: scenario.evictions,
              sets: 1000,
              deletes: 10
            },
            strategyName: scenario.strategy.toUpperCase(),
            config: { evictionPolicy: scenario.strategy }
          }),
          getMemoryUsage: jest.fn().mockReturnValue({
            currentBytes: scenario.size * 100,
            currentMB: scenario.size * 0.0001
          })
        };

        metricsService.registerCache(`scenario-cache-${i}`, mockCache, {
          category: scenario.strategy,
          description: `Scenario ${i} cache`
        });
      });

      const startTime = process.hrtime.bigint();
      
      // Perform comprehensive analysis
      const analysis = metricsService.analyzePerformance();

      const endTime = process.hrtime.bigint();
      const duration = Number(endTime - startTime) / 1000000;

      // Analysis should complete quickly even with complex data
      expect(duration).toBeLessThan(50);
      
      expect(analysis.summary.totalCaches).toBe(5);
      expect(analysis.recommendations.length).toBeGreaterThan(0);
      expect(analysis.topPerformers.length).toBeGreaterThan(0);
      expect(analysis.poorPerformers.length).toBeGreaterThan(0);

      console.log(`Performance Analysis: ${scenarios.length} caches analyzed in ${duration.toFixed(2)}ms`);
      console.log(`Generated ${analysis.recommendations.length} recommendations`);

      metricsService.destroy();
    });
  });

  describe('Memory Efficiency', () => {
    it('should maintain efficient memory usage under load', () => {
      const cache = new UnifiedCache({ logger: mockLogger }, {
        maxSize: 5000,
        maxMemoryUsage: 1024 * 1024, // 1MB limit
        evictionPolicy: 'lru'
      });

      // Track initial memory
      const initialMemory = process.memoryUsage().heapUsed;

      // Add data that should trigger memory-based eviction
      const largeDataSize = 1000; // Characters per entry
      const itemCount = 2000;

      for (let i = 0; i < itemCount; i++) {
        const largeValue = {
          id: i,
          data: Array(largeDataSize).fill(`x${i}`).join(''),
          metadata: {
            created: Date.now(),
            accessed: 0,
            version: 1
          }
        };
        cache.set(`large:item${i}`, largeValue);
      }

      // Check memory usage after loading
      const afterLoadMemory = process.memoryUsage().heapUsed;
      const memoryIncrease = afterLoadMemory - initialMemory;

      // Memory increase should be reasonable (under 10MB for this test)
      expect(memoryIncrease).toBeLessThan(10 * 1024 * 1024);

      const metrics = cache.getMetrics();
      const memoryUsage = cache.getMemoryUsage();

      // Cache should respect size limits due to memory constraints
      expect(metrics.size).toBeLessThan(itemCount);
      expect(memoryUsage.currentBytes).toBeLessThanOrEqual(1024 * 1024 * 1.1); // Allow 10% overhead

      console.log(`Memory Efficiency: ${metrics.size} items cached, ${memoryIncrease / 1024 / 1024} MB heap increase`);
      console.log(`Cache memory usage: ${memoryUsage.currentMB.toFixed(2)} MB (${memoryUsage.utilizationPercent.toFixed(1)}% of limit)`);
    });

    it('should handle memory pressure gracefully', () => {
      const cache = new UnifiedCache({ logger: mockLogger }, {
        maxSize: 10000,
        maxMemoryUsage: 512 * 1024, // Tight 512KB limit
        evictionPolicy: 'lru'
      });

      let successfulSets = 0;
      let totalAttempts = 5000;

      const startTime = process.hrtime.bigint();

      // Try to add more data than memory limit allows
      for (let i = 0; i < totalAttempts; i++) {
        const data = {
          id: i,
          payload: Array(200).fill(`data${i}`).join(''), // ~1KB per entry
          timestamp: Date.now()
        };

        cache.set(`memory:test${i}`, data);
        successfulSets++;

        // Check if we've hit memory limits
        const memoryUsage = cache.getMemoryUsage();
        if (memoryUsage.utilizationPercent > 95) {
          break;
        }
      }

      const endTime = process.hrtime.bigint();
      const duration = Number(endTime - startTime) / 1000000;

      const finalMetrics = cache.getMetrics();
      const finalMemoryUsage = cache.getMemoryUsage();

      // Should handle memory pressure without crashing
      expect(duration).toBeLessThan(1000);
      expect(finalMemoryUsage.utilizationPercent).toBeLessThanOrEqual(100);
      // Note: evictions are handled internally by lru-cache but not tracked in stats
      // The cache should have fewer items than attempts due to memory limits
      expect(finalMetrics.size).toBeLessThan(totalAttempts);

      console.log(`Memory Pressure Test: ${successfulSets}/${totalAttempts} items set, ${finalMemoryUsage.utilizationPercent.toFixed(1)}% memory utilization`);
      console.log(`Final cache size: ${finalMetrics.size}, Duration: ${duration.toFixed(2)}ms`);
    });
  });

  describe('Scalability Benchmarks', () => {
    it('should demonstrate linear scaling with cache size', () => {
      const sizes = [100, 500, 1000, 2500, 5000];
      const results = [];

      sizes.forEach(size => {
        const cache = new UnifiedCache({ logger: mockLogger }, {
          maxSize: size,
          evictionPolicy: 'lru'
        });

        const operations = size * 2; // 2x operations per cache size
        const startTime = process.hrtime.bigint();

        // Fill cache and perform operations
        for (let i = 0; i < operations; i++) {
          if (i % 4 === 0) {
            cache.set(`key${i}`, { value: i });
          } else {
            cache.get(`key${Math.floor(Math.random() * i + 1)}`);
          }
        }

        const endTime = process.hrtime.bigint();
        const duration = Number(endTime - startTime) / 1000000;
        const throughput = operations / duration * 1000;

        results.push({
          size,
          operations,
          duration,
          throughput,
          metrics: cache.getMetrics()
        });

        // Performance should remain reasonable as size increases
        expect(throughput).toBeGreaterThan(5000); // At least 5K ops/sec
      });

      console.log('\nCache Size Scalability:');
      results.forEach(result => {
        console.log(`Size ${result.size}: ${result.throughput.toFixed(0)} ops/sec (${result.duration.toFixed(2)}ms for ${result.operations} ops)`);
      });

      // Verify scaling characteristics
      const firstResult = results[0];
      const lastResult = results[results.length - 1];
      
      // Throughput shouldn't degrade significantly with size
      expect(lastResult.throughput).toBeGreaterThan(firstResult.throughput * 0.5);
    });

    it('should handle concurrent access patterns efficiently', async () => {
      const cache = new UnifiedCache({ logger: mockLogger }, {
        maxSize: 2000,
        evictionPolicy: 'lru'
      });

      const concurrency = 10;
      const operationsPerThread = 500;

      const startTime = process.hrtime.bigint();

      // Create concurrent workloads
      const promises = Array.from({ length: concurrency }, (_, threadId) => 
        Promise.resolve().then(() => {
          const threadStartTime = process.hrtime.bigint();
          
          for (let i = 0; i < operationsPerThread; i++) {
            const key = `thread${threadId}:key${i}`;
            
            if (i % 3 === 0) {
              cache.set(key, { threadId, operation: i, timestamp: Date.now() });
            } else {
              cache.get(`thread${threadId}:key${Math.floor(Math.random() * i + 1)}`);
            }
          }

          const threadEndTime = process.hrtime.bigint();
          return Number(threadEndTime - threadStartTime) / 1000000;
        })
      );

      const threadDurations = await Promise.all(promises);
      const endTime = process.hrtime.bigint();
      const totalDuration = Number(endTime - startTime) / 1000000;

      const totalOperations = concurrency * operationsPerThread;
      const throughput = totalOperations / totalDuration * 1000;

      // Should handle concurrent access efficiently
      expect(totalDuration).toBeLessThan(2000); // Under 2 seconds
      expect(throughput).toBeGreaterThan(2500); // At least 2.5K ops/sec with concurrency

      const metrics = cache.getMetrics();
      expect(metrics.stats.sets + metrics.stats.hits + metrics.stats.misses).toBe(totalOperations);

      console.log(`Concurrent Access: ${concurrency} threads × ${operationsPerThread} ops = ${throughput.toFixed(0)} ops/sec`);
      console.log(`Thread durations: min=${Math.min(...threadDurations).toFixed(2)}ms, max=${Math.max(...threadDurations).toFixed(2)}ms`);
    });
  });
});