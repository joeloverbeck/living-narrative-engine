/**
 * @file Performance tests for tag removal implementation
 * @description Tests measuring token usage improvements and processing efficiency from tag removal
 */

import {
  describe,
  beforeEach,
  afterEach,
  test,
  expect,
  jest,
} from '@jest/globals';
import { AIPromptContentProvider } from '../../src/prompting/AIPromptContentProvider.js';
import { PromptDataFormatter } from '../../src/prompting/promptDataFormatter.js';
import { SUBJECT_TYPES } from '../../src/constants/subjectTypes.js';
import { encode } from 'gpt-tokenizer';

describe('Tag Removal Performance Tests', () => {
  let promptContentProvider;
  let promptDataFormatter;
  let mockLogger;

  // Test setup helpers
  const createMockLogger = () => ({
    debug: jest.fn(),
    info: jest.fn(),
    warn: jest.fn(),
    error: jest.fn(),
  });

  const createMockServices = () => ({
    promptStaticContentService: {
      getCoreTaskDescriptionText: () => 'core-task',
      getCharacterPortrayalGuidelines: () => 'portrayal',
      getNc21ContentPolicyText: () => 'content-policy',
      getFinalLlmInstructionText: () => 'final-instructions',
    },
    perceptionLogFormatter: {
      format: (logArray) =>
        logArray.map((entry) => ({ content: entry.descriptionText || '' })),
    },
    gameStateValidationService: {
      validate: () => ({ isValid: true, errorContent: null }),
    },
    actionCategorizationService: {
      extractNamespace: jest.fn(
        (actionId) => actionId.split(':')[0] || 'unknown'
      ),
      shouldUseGrouping: jest.fn(() => false),
      groupActionsByNamespace: jest.fn(() => new Map()),
      getSortedNamespaces: jest.fn(() => []),
      formatNamespaceDisplayName: jest.fn((namespace) =>
        namespace.toUpperCase()
      ),
    },
  });

  const createTestNotesWithTags = (count = 10) => {
    return Array.from({ length: count }, (_, i) => ({
      text: `Test note ${i + 1} with detailed content for token measurement`,
      subject: `Subject ${i + 1}`,
      subjectType:
        Object.values(SUBJECT_TYPES)[i % Object.values(SUBJECT_TYPES).length],
      context: `Context ${i + 1} with additional descriptive information`,
      tags: [`tag${i + 1}`, `category${(i % 3) + 1}`, `type${(i % 2) + 1}`], // Multiple tags per note
      timestamp: new Date(2024, 0, 1, i).toISOString(),
    }));
  };

  const createTestNotesWithoutTags = (count = 10) => {
    return Array.from({ length: count }, (_, i) => ({
      text: `Test note ${i + 1} with detailed content for token measurement`,
      subject: `Subject ${i + 1}`,
      subjectType:
        Object.values(SUBJECT_TYPES)[i % Object.values(SUBJECT_TYPES).length],
      context: `Context ${i + 1} with additional descriptive information`,
      timestamp: new Date(2024, 0, 1, i).toISOString(),
    }));
  };

  const createGameStateWithNotes = (notes) => ({
    actorState: {
      components: {
        'core:notes': {
          notes: notes,
        },
      },
    },
    actorPromptData: { name: 'Test Character' },
    currentUserInput: 'Test input for performance measurement',
    perceptionLog: [],
    currentLocation: {
      name: 'Test Location',
      description: 'A test location for performance measurement',
      exits: [],
      characters: [],
    },
    availableActions: [],
  });

  beforeEach(() => {
    mockLogger = createMockLogger();
    const mockServices = createMockServices();

    promptContentProvider = new AIPromptContentProvider({
      logger: mockLogger,
      ...mockServices,
    });

    promptDataFormatter = new PromptDataFormatter({ logger: mockLogger });
  });

  afterEach(() => {
    jest.restoreAllMocks();
  });

  describe('Token Usage Improvement Validation', () => {
    test('should demonstrate token savings from tag removal in small note sets', async () => {
      const notesWithTags = createTestNotesWithTags(5);
      const notesWithoutTags = createTestNotesWithoutTags(5);

      // Simulate old behavior with tags in formatted content
      const simulatedOldFormatWithTags = notesWithTags
        .map(
          (note) => `- ${note.text} (${note.context}) [${note.tags.join(', ')}]`
        )
        .join('\n');

      // Process current notes without tags
      const gameState = createGameStateWithNotes(notesWithoutTags);
      const promptData = await promptContentProvider.getPromptData(
        gameState,
        mockLogger
      );
      const formattedData = promptDataFormatter.formatPromptData(promptData);

      // Measure token usage
      const oldTokens = encode(simulatedOldFormatWithTags).length;
      const newTokens = encode(formattedData.notesContent).length;
      const tokenSavings = oldTokens - newTokens;
      const tokenSavingsPercentage = ((tokenSavings / oldTokens) * 100).toFixed(
        1
      );

      // Validate token reduction
      expect(newTokens).toBeLessThan(oldTokens);
      expect(tokenSavings).toBeGreaterThan(0);
      expect(parseFloat(tokenSavingsPercentage)).toBeGreaterThan(3); // At least 3% savings

      // Log performance metrics for documentation
      console.log(`Token Performance Metrics (5 notes):`);
      console.log(`  - With tags: ${oldTokens} tokens`);
      console.log(`  - Without tags: ${newTokens} tokens`);
      console.log(
        `  - Savings: ${tokenSavings} tokens (${tokenSavingsPercentage}%)`
      );
    });

    test('should demonstrate significant token savings with larger note sets', async () => {
      const notesWithTags = createTestNotesWithTags(25);
      const notesWithoutTags = createTestNotesWithoutTags(25);

      // Simulate old behavior with tags
      const simulatedOldFormatWithTags = notesWithTags
        .map(
          (note) => `- ${note.text} (${note.context}) [${note.tags.join(', ')}]`
        )
        .join('\n');

      // Process current notes without tags
      const gameState = createGameStateWithNotes(notesWithoutTags);
      const promptData = await promptContentProvider.getPromptData(
        gameState,
        mockLogger
      );
      const formattedData = promptDataFormatter.formatPromptData(promptData);

      // Measure token usage
      const oldTokens = encode(simulatedOldFormatWithTags).length;
      const newTokens = encode(formattedData.notesContent).length;
      const tokenSavings = oldTokens - newTokens;
      const tokenSavingsPercentage = ((tokenSavings / oldTokens) * 100).toFixed(
        1
      );

      // Validate token reduction scales with note count
      expect(newTokens).toBeLessThan(oldTokens);
      expect(tokenSavings).toBeGreaterThan(20); // Significant savings
      expect(parseFloat(tokenSavingsPercentage)).toBeGreaterThan(5); // At least 5% savings

      // Log performance metrics for documentation
      console.log(`Token Performance Metrics (25 notes):`);
      console.log(`  - With tags: ${oldTokens} tokens`);
      console.log(`  - Without tags: ${newTokens} tokens`);
      console.log(
        `  - Savings: ${tokenSavings} tokens (${tokenSavingsPercentage}%)`
      );
    });

    test('should maintain content quality while reducing tokens', async () => {
      const notesWithoutTags = createTestNotesWithoutTags(10);
      const gameState = createGameStateWithNotes(notesWithoutTags);

      const promptData = await promptContentProvider.getPromptData(
        gameState,
        mockLogger
      );
      const formattedData = promptDataFormatter.formatPromptData(promptData);

      // Verify all essential information is preserved
      notesWithoutTags.forEach((note) => {
        expect(formattedData.notesContent).toContain(note.text);
        expect(formattedData.notesContent).toContain(note.subject);
        expect(formattedData.notesContent).toContain(note.context);
      });

      // Verify no tag-related content remains
      expect(formattedData.notesContent).not.toContain('[');
      expect(formattedData.notesContent).not.toContain('tag');
      expect(formattedData.notesContent).not.toContain('category');

      // Verify structured formatting is maintained
      expect(formattedData.notesContent).toContain('##'); // Section headers
      expect(formattedData.notesContent).toContain('###'); // Subject headers
      expect(formattedData.notesContent).toContain('- '); // List formatting
    });
  });

  describe('Processing Efficiency Validation', () => {
    test('should process notes efficiently without tag handling overhead', async () => {
      const largeNotesSet = createTestNotesWithoutTags(100);
      const gameState = createGameStateWithNotes(largeNotesSet);

      // Measure processing time
      const startTime = process.hrtime.bigint();

      const promptData = await promptContentProvider.getPromptData(
        gameState,
        mockLogger
      );
      const formattedData = promptDataFormatter.formatPromptData(promptData);

      const endTime = process.hrtime.bigint();
      const processingTimeMs = Number(endTime - startTime) / 1000000; // Convert to milliseconds

      // Validate efficient processing
      expect(processingTimeMs).toBeLessThan(200); // Should process 100 notes in under 200ms
      expect(promptData.notesArray).toHaveLength(100);
      expect(formattedData.notesContent).toContain('## Characters');

      // Verify no tags are processed
      promptData.notesArray.forEach((note) => {
        expect(note).not.toHaveProperty('tags');
      });

      console.log(
        `Processing Performance (100 notes): ${processingTimeMs.toFixed(2)}ms`
      );
    });

    test('should scale processing performance linearly with note count', async () => {
      const testSizes = [25, 50, 100, 200];
      const results = [];
      const warmupRuns = 3; // Increased warm-up runs to better mitigate JIT compilation effects
      const measurementRuns = 5; // More measurements for better median stability

      for (const noteCount of testSizes) {
        // Warm-up runs (not measured) to allow JIT compilation
        for (let warmup = 0; warmup < warmupRuns; warmup++) {
          const notes = createTestNotesWithoutTags(noteCount);
          const gameState = createGameStateWithNotes(notes);
          await promptContentProvider.getPromptData(gameState, mockLogger);
          promptDataFormatter.formatPromptData(
            await promptContentProvider.getPromptData(gameState, mockLogger)
          );
        }

        // Actual measurement runs
        const runTimes = [];
        let validationData = null;

        for (let run = 0; run < measurementRuns; run++) {
          const notes = createTestNotesWithoutTags(noteCount);
          const gameState = createGameStateWithNotes(notes);

          const startTime = process.hrtime.bigint();
          const promptData = await promptContentProvider.getPromptData(
            gameState,
            mockLogger
          );
          const formattedData =
            promptDataFormatter.formatPromptData(promptData);
          const endTime = process.hrtime.bigint();

          const processingTimeMs = Number(endTime - startTime) / 1000000;
          runTimes.push(processingTimeMs);

          // Store first run data for validation
          if (run === 0) {
            validationData = { promptData, formattedData };
          }
        }

        // Validate processing success after measurements
        expect(validationData.promptData.notesArray).toHaveLength(noteCount);
        expect(
          validationData.formattedData.notesContent.length
        ).toBeGreaterThan(0);

        // Use median time to exclude outliers
        runTimes.sort((a, b) => a - b);
        const medianTime = runTimes[Math.floor(measurementRuns / 2)];
        const timePerNote = medianTime / noteCount;

        results.push({
          noteCount,
          processingTimeMs: medianTime,
          timePerNote,
        });
      }

      // Validate linear scaling (time per note should be roughly consistent)
      const timesPerNote = results.map((r) => r.timePerNote);
      const maxTimePerNote = Math.max(...timesPerNote);
      const minTimePerNote = Math.min(...timesPerNote);
      const varianceRatio = maxTimePerNote / minTimePerNote;

      // Relaxed threshold (15x) as this is a micro-benchmark with high variability
      // The test verifies the code still processes efficiently, not strict linear scaling
      // which is difficult to achieve in JavaScript with small datasets (<1ms operations)
      expect(varianceRatio).toBeLessThan(15); // Relaxed variance for micro-benchmark stability

      console.log('Processing Performance Scaling:');
      results.forEach((result) => {
        console.log(
          `  - ${result.noteCount} notes: ${result.processingTimeMs.toFixed(2)}ms (${result.timePerNote.toFixed(3)}ms/note)`
        );
      });
    });
  });

  describe('Response Time Integration Validation', () => {
    test('should measure end-to-end workflow response time improvements', async () => {
      const notesSet = createTestNotesWithoutTags(50);
      const gameState = createGameStateWithNotes(notesSet);

      // Measure complete workflow time
      const startTime = Date.now();

      // Complete workflow: extraction → formatting → section generation
      const promptData = await promptContentProvider.getPromptData(
        gameState,
        mockLogger
      );
      const formattedData = promptDataFormatter.formatPromptData(promptData);
      const notesSection = promptDataFormatter.formatNotesSection(
        promptData.notesArray,
        { groupBySubject: true }
      );

      const endTime = Date.now();
      const totalWorkflowTime = endTime - startTime;

      // Validate efficient end-to-end processing
      expect(totalWorkflowTime).toBeLessThan(100); // Under 100ms for complete workflow
      expect(notesSection).toContain('<notes>');
      expect(notesSection).toContain('## Characters');
      expect(formattedData.notesContent.length).toBeGreaterThan(0);

      console.log(`End-to-End Workflow (50 notes): ${totalWorkflowTime}ms`);
    });

    test('should maintain consistent response times under load', async () => {
      const iterations = 5;
      const noteCount = 75;
      const responseTimes = [];

      for (let i = 0; i < iterations; i++) {
        const notes = createTestNotesWithoutTags(noteCount);
        const gameState = createGameStateWithNotes(notes);

        const startTime = Date.now();
        const promptData = await promptContentProvider.getPromptData(
          gameState,
          mockLogger
        );
        const formattedData = promptDataFormatter.formatPromptData(promptData);
        const endTime = Date.now();

        const responseTime = endTime - startTime;
        responseTimes.push(responseTime);

        // Validate processing success
        expect(promptData.notesArray).toHaveLength(noteCount);
        expect(formattedData.notesContent.length).toBeGreaterThan(0);
      }

      // Calculate statistics
      const avgResponseTime =
        responseTimes.reduce((sum, time) => sum + time, 0) / iterations;
      const maxResponseTime = Math.max(...responseTimes);
      const minResponseTime = Math.min(...responseTimes);
      const responseTimeVariance = maxResponseTime - minResponseTime;

      // Validate consistent performance
      expect(avgResponseTime).toBeLessThan(150); // Average under 150ms
      expect(responseTimeVariance).toBeLessThan(100); // Variance under 100ms

      console.log(
        `Response Time Consistency (${noteCount} notes, ${iterations} iterations):`
      );
      console.log(`  - Average: ${avgResponseTime.toFixed(2)}ms`);
      console.log(`  - Range: ${minResponseTime}ms - ${maxResponseTime}ms`);
      console.log(`  - Variance: ${responseTimeVariance}ms`);
    });
  });

  describe('Backwards Compatibility Performance', () => {
    test('should handle mixed legacy and structured notes efficiently', async () => {
      const mixedNotes = [
        // Legacy notes (just text)
        { text: 'Legacy note 1' },
        { text: 'Legacy note 2' },
        // Structured notes without tags
        {
          text: 'Structured note 1',
          subject: 'Subject 1',
          subjectType: SUBJECT_TYPES.CHARACTER,
          context: 'context 1',
        },
        // Simulated legacy with tags (should be filtered)
        {
          text: 'Legacy with tags',
          subject: 'Subject 2',
          subjectType: SUBJECT_TYPES.LOCATION,
          tags: ['legacy', 'filtered'], // These would be filtered out
        },
      ];

      const gameState = createGameStateWithNotes(mixedNotes);
      const startTime = Date.now();

      const promptData = await promptContentProvider.getPromptData(
        gameState,
        mockLogger
      );
      const formattedData = promptDataFormatter.formatPromptData(promptData);

      const endTime = Date.now();
      const processingTime = endTime - startTime;

      // Validate efficient mixed processing
      expect(processingTime).toBeLessThan(50); // Fast processing even with mixed formats
      expect(promptData.notesArray).toHaveLength(4);

      // Verify tags are not present in any processed notes
      promptData.notesArray.forEach((note) => {
        expect(note).not.toHaveProperty('tags');
      });

      // Verify content formatting handles mixed types
      expect(formattedData.notesContent).toContain('Legacy note 1');
      expect(formattedData.notesContent).toContain('Structured note 1');
      expect(formattedData.notesContent).toContain('Legacy with tags');

      console.log(`Mixed Notes Processing: ${processingTime}ms`);
    });
  });

  describe('Performance Regression Prevention', () => {
    test('should establish performance baseline for future validation', async () => {
      const baselineNoteCount = 30;
      const notes = createTestNotesWithoutTags(baselineNoteCount);
      const gameState = createGameStateWithNotes(notes);

      // Establish performance baselines
      const measurements = {
        extraction: 0,
        formatting: 0,
        totalWorkflow: 0,
        tokenCount: 0,
      };

      // Measure extraction phase
      let start = process.hrtime.bigint();
      const promptData = await promptContentProvider.getPromptData(
        gameState,
        mockLogger
      );
      let end = process.hrtime.bigint();
      measurements.extraction = Number(end - start) / 1000000;

      // Measure formatting phase
      start = process.hrtime.bigint();
      const formattedData = promptDataFormatter.formatPromptData(promptData);
      end = process.hrtime.bigint();
      measurements.formatting = Number(end - start) / 1000000;

      // Calculate total workflow time
      measurements.totalWorkflow =
        measurements.extraction + measurements.formatting;

      // Measure token usage
      measurements.tokenCount = encode(formattedData.notesContent).length;

      // Establish baselines (these thresholds should not be exceeded in future tests)
      const baselines = {
        maxExtractionTime: 50, // ms
        maxFormattingTime: 30, // ms
        maxTotalWorkflowTime: 80, // ms
        maxTokensPerNote: 35, // tokens per note
      };

      // Validate against baselines
      expect(measurements.extraction).toBeLessThan(baselines.maxExtractionTime);
      expect(measurements.formatting).toBeLessThan(baselines.maxFormattingTime);
      expect(measurements.totalWorkflow).toBeLessThan(
        baselines.maxTotalWorkflowTime
      );
      expect(measurements.tokenCount / baselineNoteCount).toBeLessThan(
        baselines.maxTokensPerNote
      );

      // Log baseline metrics for future reference
      console.log(`Performance Baselines (${baselineNoteCount} notes):`);
      console.log(`  - Extraction: ${measurements.extraction.toFixed(2)}ms`);
      console.log(`  - Formatting: ${measurements.formatting.toFixed(2)}ms`);
      console.log(
        `  - Total Workflow: ${measurements.totalWorkflow.toFixed(2)}ms`
      );
      console.log(
        `  - Tokens: ${measurements.tokenCount} (${(measurements.tokenCount / baselineNoteCount).toFixed(1)} per note)`
      );
    });
  });
});
